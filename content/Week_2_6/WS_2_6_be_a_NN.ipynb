{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 14: Be like a Neural Network\n",
    "\n",
    "<h1 style=\"position: absolute; display: flex; flex-grow: 0; flex-shrink: 0; flex-direction: row-reverse; top: 60px;right: 30px; margin: 0; border: 0\">\n",
    "    <style>\n",
    "        .markdown {width:100%; position: relative}\n",
    "        article { position: relative }\n",
    "    </style>\n",
    "    <img src=\"https://gitlab.tudelft.nl/mude/public/-/raw/main/tu-logo/TU_P1_full-color.png\" style=\"width:100px\" />\n",
    "    <img src=\"https://gitlab.tudelft.nl/mude/public/-/raw/main/mude-logo/MUDE_Logo-small.png\" style=\"width:100px\" />\n",
    "</h1>\n",
    "<h2 style=\"height: 10px\">\n",
    "</h2>\n",
    "\n",
    "*[CEGM1000 MUDE](http://mude.citg.tudelft.nl/): Week 2.6. Wednesday December 20, 2023.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.pinimg.com/originals/e5/6e/8a/e56e8a055bfbcbeafaf413a70c911876.jpg\" width=\"500\" height=\"400\">\n",
    "\n",
    "Source image: https://www.kaggle.com/code/pranavkasela/neural-networks-from-scratch-code-maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "For several tasks within the domain of Civil Engineernig and Geosciences we might be interested in applying machine learning models to understand some physical processes and phenomena which, for many different reasons, are difficult to interpret. Training a machine learning model will typically involves the following steps:\n",
    "\n",
    "1. Define the model.\n",
    "\n",
    "2. Define the loss function.\n",
    "\n",
    "3. Train the model.\n",
    "\n",
    "4. Evaluate the model.\n",
    "\n",
    "By using these predefined functions, we can save time and effort when building and training machine learning models.\n",
    "\n",
    "In this notebook, we start with a very simple dataset with an underlying linear pattern. We then train a simple Multilayer Perceptron (MLP) on it and discuss matters related to model complexity and overfitting. Finally, we move to a more realistic dataset you have already seen before during MUDE.\n",
    "\n",
    "### Python Environment\n",
    "\n",
    "You will need the package scikit-learn for this workshop (in addition to a few other typical packages). You can import it to one of your existing conda environments from the conda-forge as (i.e., `\n",
    "conda install -c conda-forge scikit-learn`), or you can create a new environment from the `*.yml` file included in this repository (`conda env create -f environment_MUDE_ml.yml`). But remember: _if you already have sklearn installed in an environment, you don't have to do anything besides use it!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C8FFFF; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Hint:</b>   \n",
    "\n",
    "We will use boxes with this type of formatting to introduce you to the <code>scikit-learn</code> package and help you with some programming tips.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import interpolate\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 0: Our Data for Today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models learn from data and therefore we need data to train our models. For this assignment we will exemplify the use of machine learning in a simple dataset. First, let's create these dummy data. \n",
    "\n",
    "<code>data_x</code> represents a general independent variable which has a linear relationship with a target variable $y$. In this case, the ground truth $t$ is:\n",
    "\n",
    "$$\n",
    "t = 0.8x+4.75\n",
    "$$\n",
    "\n",
    "To make the problem a bit more interesting (and realistic!), we also add Gaussian noise with unit variance to our observations:\n",
    "\n",
    "$$\n",
    "t = 0.8x+4.75+\\epsilon\\quad\\quad\\epsilon\\sim\\mathcal{N}(0,1)\n",
    "$$\n",
    "\n",
    "Finally, we introduce a _dense validation dataset_ to evaluate our model complexity. Normally you would **split the original dataset** into training and validation sets (as done in PA14), but since our dataset is very small this is not a feasible strategy. For this demonstration we will instead simply evaluate the actual loss through dense integration with a `linspace`:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[L]=\\displaystyle\\int\\int\\left(t-y(x,\\mathbf{w})\\right)^2p(x,t)\\,dx\\,dt\n",
    "\\approx\n",
    "\\displaystyle\\frac{1}{N_\\mathrm{val}}\\sum_{n=1}^{N_\\mathrm{val}}\\left(t_n-y(x_n)\\right)^2\n",
    "$$\n",
    "\n",
    "As you can see, this expected loss is based on the squared error, where $t$ is the true value and $y(x)$ is our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Task 0:</b>  \n",
    "\n",
    "Read the code below, making sure you understand what the data are, as well as the validation set and how it is created. Then execute the cells to visualize the data.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some data\n",
    "np.random.seed(42)\n",
    "noise_level = 1.0\n",
    "\n",
    "data_x = np.array([[1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0]]).transpose()\n",
    "data_t = 0.8 * data_x + 4.75 + np.random.normal(scale=noise_level,size=data_x.shape)\n",
    "\n",
    "# Get a very dense validation set to give us the real loss\n",
    "\n",
    "x_val = np.linspace(np.min(data_x),np.max(data_x),1000)\n",
    "t_val = 0.8*x_val + 4.75 + np.random.normal(scale=noise_level,size=x_val.shape)\n",
    "x_val = x_val.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(data_x.flatten(), data_t, 'x', color='blue', markersize=10, label='Data')\n",
    "ax.set_title('Linear Data Example', fontsize=16)\n",
    "ax.set_xlabel('x', fontsize=14)\n",
    "ax.set_ylabel('t', fontsize=14)\n",
    "ax.legend(fontsize=14)\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Create Your First Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px\">\n",
    "<p>\n",
    "<b>Task 1.1: Implement an MLP</b>   \n",
    "\n",
    "We now try to fit this data with a Multilayer Perceptron (MLP), also known as a Feedforward Neural Network (FNN), with input, hidden and output layers, each with a specified number of neurons. For such models we also need to specify some hyperparameters. In Scikit-learn, the MLP is defined in the <code>MLPRegressor</code> class, you can see the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html).\n",
    "You should start with a linear MLP, so with `identity` activation and without hidden layers.\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C8FFFF; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Hint:</b>   \n",
    "\n",
    "You can create the model using the method and two arguments.\n",
    "\n",
    "Specifying <code>hidden_layer_sizes=()</code> implies that the Neural Network does not have hidden layers. There is only one input layer and one output layer and therefore it is transforming directly $x$ into $y$ without going through intermediate steps.\n",
    "\n",
    "Specifying <code>activation = 'identity'</code> means that we are not going to alter the output of the model with an activation function such as ReLU, tanh, or sigmoid (the most popular activation functions for Neural Networks).\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px\">\n",
    "<p>\n",
    "<b>Task 1.2: Train the MLP</b>   \n",
    "\n",
    "So far the model has not been trained yet. This is something we can do using the <code>partial_fit</code> method and then make predictions with the <code>predict</code> method. Fill in the code below to find the model predictions for the training and validation sets (defined above)\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10000\n",
    "N_print = 10**(int(np.log10(n_epochs)) - 1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.partial_fit(data_x, data_t.flatten())\n",
    "\n",
    "    MLP_prediction = \n",
    "    MLP_valprediction = \n",
    "    \n",
    "    if epoch%N_print==0 or epoch==n_epochs-1: \n",
    "        print((f'Epoch: {epoch:6d}/{n_epochs}, '\n",
    "               + f'MSE: {mean_squared_error(data_t, MLP_prediction.reshape(-1,1)):0.4f}, '\n",
    "               + f'Real loss: {mean_squared_error(t_val,MLP_valprediction):0.4f}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C8FFFF; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Hint:</b>   \n",
    "\n",
    "<b>Be careful about re-running cells!</b> If you executed the cell above more than once, you may have noticed that the values of loss sand MSE stopped changing. Note carefully that in the for loop above we are operating on <code>model</code>, which is an object with type <code>sklearn.neural_network._multilayer_perceptron.MLPRegressor</code>. You can \"ask\" the model about its status by checking how many epochs have been evaluated with the <code>t_</code> attribute (try it!). If you need to \"reset\" the model, simply redefine the variable <code>model</code>.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the loss function progressively decreases. That means that our model is indeed learning! That is, it is reducing its error.\n",
    "\n",
    "Also notice how the `Real loss` value decreases with time. Again remember this is the value of the loss function obtained with a very dense validation dataset. Notice how the training loss is usually lower than the real loss. This is expected, as the training loss is obtained with a very small dataset and is therefore overly optimistic (on average)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the predictions from our model against the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(data_x, data_t, \".\", markersize=20, label=\"Data\")\n",
    "ax.plot(data_x, MLP_prediction, \"-o\", markersize=10, label=\"Prediction\")\n",
    "\n",
    "# Add a title and axis labels\n",
    "ax.set_title(\"Linear Data Example\", fontsize=16)\n",
    "ax.set_xlabel(\"x\", fontsize=14)\n",
    "ax.set_ylabel(\"t\", fontsize=14)\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "# Add a grid\n",
    "ax.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then print the weights of the model. How do they compare to the original slope (0.8) and intercept (4.75)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Model coefficients: {model.coefs_}')\n",
    "print(f'Model intercepts: {model.intercepts_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px\">\n",
    "<p>\n",
    "<b>Task 1.3: Increase MLP complexity</b>   \n",
    "\n",
    "Now let us see if it is a good idea to use a much more flexible model. Initialize and train another MLP, but this time with **five hidden layers with 50 units each** and **ReLU** activation.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOUR_CODE_HERE\n",
    "\n",
    "n_epochs = 10000\n",
    "N_print = 10**(int(np.log10(n_epochs)) - 1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.partial_fit(data_x, data_t.flatten())\n",
    "\n",
    "    MLP_prediction = YOUR CODE HERE\n",
    "    MLP_valprediction = YOUR CODE HERE\n",
    "    \n",
    "    if epoch%N_print==0 or epoch==n_epochs-1: \n",
    "        print((f'Epoch: {epoch:6d}/{n_epochs}, '\n",
    "               + f'MSE: {mean_squared_error(data_t, MLP_prediction.reshape(-1,1)):0.4f}, '\n",
    "               + f'Real loss: {mean_squared_error(t_val,MLP_valprediction):0.4f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(data_x, data_t, \".\", markersize=20, label=\"Data\")\n",
    "ax.plot(data_x, MLP_prediction, \"-o\", markersize=10, label=\"Prediction\")\n",
    "\n",
    "# Add a title and axis labels\n",
    "ax.set_title(\"Linear Data Example\", fontsize=16)\n",
    "ax.set_xlabel(\"x\", fontsize=14)\n",
    "ax.set_ylabel(\"t\", fontsize=14)\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "# Add a grid\n",
    "ax.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened to the real loss? Is it higher or lower than before? Is this model good? Think about it: we know the ground truth is actually linear. Would it make sense to have such a flexible model here? Is the validation loss giving you a valuable hint about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px\">\n",
    "<p>\n",
    "<b>Task 1.4: Control MLP complexity</b>   \n",
    "\n",
    "The previous model is way too flexible and we ended up overfitting the noise in the data. So we should use a smaller model, but tweaking the architecture can be annoying. \n",
    "\n",
    "For this part you can try something different: **Keep the same network size but add an $L_2$ regularization term** to your model. In `scikit-learn` you can do this by setting the `alpha` parameter you give to `MLPRegressor` to your desired value of $\\lambda$. \n",
    "\n",
    "Try different values in the interval $0<\\lambda\\leq1$ and see what happens to model complexity. **What is the value of $\\lambda$ that leads to the lowest value for the real loss?**\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOUR_CODE_HERE\n",
    "\n",
    "n_epochs = 10000\n",
    "N_print = 10**(int(np.log10(n_epochs)) - 1)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.partial_fit(data_x, data_t.flatten())\n",
    "\n",
    "    MLP_prediction = YOUR CODE HERE\n",
    "    MLP_valprediction = YOUR CODE HERE\n",
    "    \n",
    "    if epoch%N_print==0 or epoch==n_epochs-1: \n",
    "        print((f'Epoch: {epoch:6d}/{n_epochs}, '\n",
    "               + f'MSE: {mean_squared_error(data_t, MLP_prediction.reshape(-1,1)):0.4f}, '\n",
    "               + f'Real loss: {mean_squared_error(t_val,MLP_valprediction):0.4f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(data_x, data_t, \".\", markersize=20, label=\"Data\")\n",
    "ax.plot(data_x, MLP_prediction, \"-o\", markersize=10, label=\"Prediction\")\n",
    "\n",
    "# Add a title and axis labels\n",
    "ax.set_title(\"Linear Data Example\", fontsize=16)\n",
    "ax.set_xlabel(\"x\", fontsize=14)\n",
    "ax.set_ylabel(\"t\", fontsize=14)\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(fontsize=14)\n",
    "\n",
    "# Add a grid\n",
    "ax.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close is the model with the lowest validation loss to your first linear model? Is this a reassuring result? \n",
    "\n",
    "Neural networks can be very flexible and that is usually a good thing. But at the end of the day you want a model that performs **your specific task** as well as possible, and sometimes that means a very simple linear model is all what you need.\n",
    "\n",
    "It is nice to see that even if we do not actually know how complex the underlying patterns in our data are, if we have **a good validation dataset** we can rely on it to tell us how flexible our model should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Predict land deformation\n",
    "\n",
    "Let's go back to what you did in Week 1.3 - Observation Theory. In Project 2 you were asked to model the deformation of a road caused by the subsidence of the underground by means of GNSS and InSAR data. \n",
    "\n",
    "In this workshop you will focus only on the GNSS to making predictions about land deformation. You will be asked to:\n",
    "\n",
    "- Create a neural network similar to the one you created in the previous tasks\n",
    "- Change some network parameters and observe how results are affected.\n",
    "- Compare the results with those from Project 2, where you applied BLUE.\n",
    "\n",
    "Although we focus here on building and evaluating a neural network, the greater MUDE purpose is to think about the following questions (for both types of models!!!):\n",
    "\n",
    "- Which model is the best?\n",
    "- Are the Neural Network and BLUE able to capture the trend of the data?\n",
    "- What parameters affect the the results most?\n",
    "- What are the differences between the two approaches? In which situation would one be more effective compared to the other?\n",
    "\n",
    "### Task 2.0: Data Processing\n",
    "\n",
    "First we need to import and convert our data properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnss = pd.read_csv('./data/gnss_observations2.csv')\n",
    "dates_gnss = pd.to_datetime(gnss['dates'])\n",
    "gnss_obs = (gnss['observations[m]']).to_numpy() * 1000\n",
    "\n",
    "def to_days_years(dates):\n",
    "    '''Convert the observation dates to days and years.'''\n",
    "    \n",
    "    dates_datetime = pd.to_datetime(dates)\n",
    "    time_diff = (dates_datetime - dates_datetime[0])\n",
    "    days_diff = (time_diff / np.timedelta64(1,'D')).astype(int)\n",
    "    \n",
    "    days = days_diff.to_numpy()\n",
    "    \n",
    "    return days\n",
    "\n",
    "days_gnss = to_days_years(dates_gnss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(days_gnss, gnss_obs, 'o', mec='black', label = 'GNSS')\n",
    "plt.legend()\n",
    "plt.title('GNSS observations of land deformation')\n",
    "plt.ylabel('Displacement [mm]')\n",
    "plt.xlabel('Time [days]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Task 2.1: Split the data</b>  \n",
    "\n",
    "Define what <code>X</code> and <code>t</code> are. \n",
    "\n",
    "Then use the function <code>train_test_split</code> from <code>sklearn.model_selection</code> library to split the initial dataset into a training and validation dataset. Check the [documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). (this is what you did in PA14, manually)\n",
    "\n",
    "Use **80%** and **20%** of the data for training and validation, respectively. Also make sure that <code>random_state = 42</code>.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C8FFFF; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Hint:</b>   \n",
    "\n",
    "You will need to reshape your arrays in order to first standardize the data and consequently train the model. To do so check [this function](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html). The new shape should be <code>(-1,1)</code>. We do this for you ;)\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = days_gnss.reshape(-1, 1)\n",
    "t = gnss_obs.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, t_train, t_val  = YOUR_CODE_HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(X_train, t_train, 'o', mec='green', label = 'Training')\n",
    "plt.plot(X_val, t_val, 'o', mec='blue', label = 'Validation')\n",
    "plt.title('GNSS observations of land deformation - training and validation datasets')\n",
    "plt.legend()\n",
    "plt.ylabel('Displacement [mm]')\n",
    "plt.xlabel('Time [days]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Task 2.2: Normalize the data</b>  \n",
    "\n",
    "Before training the model you need to standardize your data. To do so you need <code>StandardScaler</code> from <code>sklearn.preprocessing</code> library. Check the [documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). \n",
    "\n",
    "Make sure to standardize both <code>X</code> datasets and save them in different arrays. \n",
    "\n",
    "Using **a different** `StandardScaler`, normalize the outputs `t_train` and `t_val` as well.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#C8FFFF; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Hint:</b>   \n",
    "\n",
    "You first need to standardize just <code>X_train</code> and `t_train`. Only then you can standardize the validation dataset. This will guarantee that the validation data is normalized in the same way as the training data, othersize the network will get confused when it is time to make predictions. We do this for you ;)\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_scaler = StandardScaler()\n",
    "target_scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = input_scaler.fit_transform(X_train)\n",
    "X_val_scaled = input_scaler.transform(X_val)\n",
    "\n",
    "t_train_scaled = target_scaler.fit_transform(t_train)\n",
    "t_val_scaled = target_scaler.transform(t_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(X_train_scaled, t_train_scaled, 'o', mec='green', label = 'Training')\n",
    "plt.plot(X_val_scaled, t_val_scaled, 'o', mec='blue', label = 'Validation')\n",
    "plt.title('Normalized GNSS dataset')\n",
    "plt.legend()\n",
    "plt.ylabel('Normalized displacement [-]')\n",
    "plt.xlabel('Normalized time [-]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Task 2.2B: Check the Figure!</b>  \n",
    "\n",
    "Look at the figure above and notice how the data has been changed by the normalization process.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Task 2.3: Create a linear MLP</b>  \n",
    "\n",
    "As you have done in Task 1 using <code>MLPRegressor</code>, create the Neural Network with no hidden layers and <code>identity</code> activation function. \n",
    "\n",
    "In the <code>MLPRegressor</code> you can specify several hyperparameters. Some of the default values are <code>solver='adam'</code> for the optimizer and <code>learning_rate_init=0.001</code> for the initial learning rate $\\eta$. For the moment we keep these values fixed, but later you can try to change some of them and see what you get.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gnss = YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Task 2.4: Train the MLP</b>  \n",
    "\n",
    "Now you can effectively train the model and then test it. In both cases you want to compute the loss and save it.\n",
    "\n",
    "You are required to do the following:\n",
    "<ol>\n",
    "    <li>Initialize the lists for saving the training and validation loss</li>\n",
    "    <li>Loop over the epochs to train and validate the model. The line <code>model_gnss.partial_fit(X_train_scaled, t_train)</code> is used for training the model in full batch mode. Check the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor.partial_fit\" target=\"_blank\">Documentation</a> if you're interested.</li>\n",
    "    <li>For both steps compute the Mean Squared Error (<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn-metrics-mean-squared-erro\" target=\"_blank\">documentation here</a>) and store it in the lists you previously initialized.</li>\n",
    "</ol>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "epochs = YOUR CODE HERE\n",
    "\n",
    "for epoch in range(YOUR CODE HERE):\n",
    "    model_gnss.partial_fit(X_train_scaled, t_train_scaled.flatten())\n",
    "\n",
    "    # Calculate training loss\n",
    "    train_pred = YOUR CODE HERE\n",
    "    train_loss = YOUR CODE HERE\n",
    "    train_losses.YOUR CODE HERE\n",
    "\n",
    "    # Calculate validation loss\n",
    "    val_pred = YOUR CODE HERE\n",
    "    val_loss = YOUR CODE HERE\n",
    "    val_losses.YOUR CODE HERE\n",
    "\n",
    "    # Print losses every 500 epochs\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch}/{epochs} - Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you can plot the losses and the predictions made by the Neural Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label='Training Loss', c='b')\n",
    "plt.plot(val_losses, label='Validation Loss', c='r')\n",
    "plt.title('Training, Validation, and Test Losses over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dataset\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(X_train, t_train, 'o', mec='green', label = 'Training')\n",
    "plt.plot(X_val, t_val, 'o', mec='blue', label = 'Validation')\n",
    "\n",
    "# Get model predictions for a dense linspace in x\n",
    "x_plot = np.linspace(np.min(X),np.max(X),1000).reshape(-1,1)\n",
    "y_plot = model_gnss.predict(input_scaler.transform(x_plot))\n",
    "plt.plot(x_plot,target_scaler.inverse_transform(y_plot.reshape(-1,1)),color='orange',linewidth=5,label='Network predictions')\n",
    "\n",
    "# plt.plot(, target_scaler.inverse_transform(val_pred.reshape(-1,1)), 'x', mec='red', label='Predicted Values')\n",
    "plt.title('Obvserved vs Predicted Values')\n",
    "plt.ylabel('Displacement [mm]')\n",
    "plt.xlabel('Time [days]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we had to **carefully normalize and de-normalize** our data when making new predictions. The network is used to seeing normalized inputs and producing normalized outputs, so we have to:\n",
    "\n",
    "- Create a `linspace` with the new locations we want to predict at;\n",
    "- Normalize these inputs with `input_scaler.transform()` so that they fall within the range the network was trained for;\n",
    "- Call `predict()` to make normalized predictions of $y(x)$;\n",
    "- Bring the predictions back to the real space with `target_scaler.inverse_transform()` in order to plot them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Task 2.5: Use a more complex network</b>  \n",
    "\n",
    "Now it is time to make the network a bit more complex. You can change different hyperparameters as we have seen. See [documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) for the next tasks.\n",
    "Go back to the model and do the following:\n",
    "<ol>\n",
    "    <li>Add hidden layers to your network</li>\n",
    "    <li>Change activation functions. Try for instance using ReLU or Tanh</li>\n",
    "    <li>Try different combinations of activations and network architecture. The ReLU activation tends to work better with deeper networks</li>\n",
    "    <li>Adjust the strength of the $L_2$ regularization by tweaking the alpha hyperparameter</li>\n",
    "</ol>\n",
    "\n",
    "Then run the model again as many times as you deem necessary. Then look at the validation error and use what you have learned before: what is the best model?\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "epochs = YOUR CODE HERE\n",
    "\n",
    "for epoch in range(YOUR CODE HERE):\n",
    "    model_gnss.partial_fit(X_train_scaled, t_train_scaled.flatten())\n",
    "\n",
    "    # Calculate training loss\n",
    "    train_pred = YOUR CODE HERE\n",
    "    train_loss = YOUR CODE HERE\n",
    "    train_losses.YOUR CODE HERE\n",
    "\n",
    "    # Calculate validation loss\n",
    "    val_pred = YOUR CODE HERE\n",
    "    val_loss = YOUR CODE HERE\n",
    "    val_losses.YOUR CODE HERE\n",
    "\n",
    "    # Print losses every 500 epochs\n",
    "    if epoch % 500 == 0:\n",
    "        print(f'Epoch {epoch}/{epochs} - Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label='Training Loss', c='b')\n",
    "plt.plot(val_losses, label='Validation Loss', c='r')\n",
    "plt.title('Training and Validation Losses over Epochs with a better model')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dataset\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(X_train, t_train, 'o', mec='green', label = 'Training')\n",
    "plt.plot(X_val, t_val, 'o', mec='blue', label = 'Validation')\n",
    "\n",
    "# Get model predictions for a dense linspace in x\n",
    "x_plot = np.linspace(np.min(X),np.max(X),1000).reshape(-1,1)\n",
    "y_plot = new_model_gnss.predict(input_scaler.transform(x_plot))\n",
    "plt.plot(x_plot,target_scaler.inverse_transform(y_plot.reshape(-1,1)),color='orange',linewidth=5,label='Network predictions')\n",
    "\n",
    "plt.title('Obvserved vs Predicted Values')\n",
    "plt.ylabel('Displacement [mm]')\n",
    "plt.xlabel('Time [days]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Compare with _Project 2 - Modelling Road Deformation using Non-Linear Least-Squares_\n",
    "\n",
    "The following cell contains the code you implemented in Project 2 during Week 1.3; you can review the solution [here](https://mude.citg.tudelft.nl/course-files/Project_2/Solution.html) if you need a reminder. For this application we only focus on the linear model implemented using **BLUE**. You just need to run the following cell and then compare the results, i.e. the residuals, with respect to the Neural Network model you have previously implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Task 3.1: Model Comparison</b>  \n",
    "\n",
    "Scan through the code quickly to refresh your memore about the BLUE method and how we used it to solve this problem in Q1. THen run the cells below and answer the questions (also below).\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gw = pd.read_csv('./data/groundwater_levels2.csv')\n",
    "dates_gw = pd.to_datetime(gw['dates'])\n",
    "gw_obs = (gw['observations[mm]']).to_numpy()\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\n",
    "# Function to convert to days and years \n",
    "\n",
    "def to_days_years(dates):\n",
    "    '''Convert the observation dates to days and years.'''\n",
    "    \n",
    "    dates_datetime = pd.to_datetime(dates)\n",
    "    time_diff = (dates_datetime - dates_datetime[0])\n",
    "    days_diff = (time_diff / np.timedelta64(1,'D')).astype(int)\n",
    "    \n",
    "    days = days_diff.to_numpy()\n",
    "    years = days/365\n",
    "    \n",
    "    return days, years\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\n",
    "# Convert data of GNSS and GW levels\n",
    "\n",
    "days_gnss, years_gnss = to_days_years(dates_gnss)\n",
    "days_gw, years_gw = to_days_years(dates_gw)\n",
    "\n",
    "interp = interpolate.interp1d(days_gw, gw_obs)\n",
    "\n",
    "GW_at_GNSS_times = interp(days_gnss)\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\n",
    "# Functional model for GNSS: A_gnss\n",
    "\n",
    "A_gnss = np.ones((len(dates_gnss), 3))\n",
    "A_gnss[:,1] = days_gnss\n",
    "A_gnss[:,2] = GW_at_GNSS_times\n",
    "\n",
    "y_gnss = gnss_obs\n",
    "\n",
    "m_gnss = np.shape(A_gnss)[0]\n",
    "n_gnss = np.shape(A_gnss)[1]\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\n",
    "# Stochastic model for GNSS: Sigma_Y_gnss\n",
    "\n",
    "std_gnss = 15 #mm (corrected from original value of 5 mm)\n",
    "\n",
    "Sigma_Y_gnss = np.identity(len(dates_gnss))*std_gnss**2\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\n",
    "# BLUE function - not needed if non-linear model is used!\n",
    "\n",
    "def BLUE(A, y, Sigma_Y):\n",
    "    \"\"\"Calculate the Best Linear Unbiased Estimator\n",
    "    \n",
    "    Write a docstring here (an explanation of your function).\n",
    "    \n",
    "    Function to calculate the Best Linear Unbiased Estimator\n",
    "    \n",
    "    Input:\n",
    "        A = A matrix (mxn)\n",
    "        y = vector with obervations (mx1)\n",
    "        Sigma_Y = Varaiance covariance matrix of the observations (mxm)\n",
    "    \n",
    "    Output:\n",
    "        xhat = vector with the estimates (nx1)\n",
    "        Sigma_Xhat = variance-covariance matrix of the unknown parameters (nxn)\n",
    "    \"\"\"\n",
    "    \n",
    "    Sigma_Xhat = np.linalg.inv(A.T @ np.linalg.inv(Sigma_Y) @ A)\n",
    "    xhat = Sigma_Xhat @ A.T @ np.linalg.inv(Sigma_Y) @ y\n",
    "    \n",
    "    return xhat, Sigma_Xhat \n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\n",
    "# BLUE estimation \n",
    "\n",
    "xhat_gnss, Sigma_Xhat_gnss = BLUE(A_gnss, y_gnss, Sigma_Y_gnss)\n",
    "\n",
    "\n",
    "# Function to plot BLUE residuals\n",
    "\n",
    "def plot_residual(date, y_obs, yhat, data_type, A,\n",
    "                  Sigma_Xhat, Sigma_Y, true_disp):\n",
    "\n",
    "    ehat = y_obs - yhat\n",
    "\n",
    "    # Compute the vc matrix for \\hat{y}\n",
    "    Sigma_Yhat = A @ Sigma_Xhat @ A.T\n",
    "    std_y = np.sqrt(Sigma_Yhat.diagonal())\n",
    "\n",
    "    # Compute the vc matrix for \\hat{e}\n",
    "    Sigma_ehat = Sigma_Y - Sigma_Yhat\n",
    "    std_ehat = np.sqrt(Sigma_ehat.diagonal())\n",
    "\n",
    "    # Show the 99% confidence interval\n",
    "    k99 = norm.ppf(1 - 0.5*0.01)\n",
    "    confidence_interval_y = k99*std_y\n",
    "    confidence_interval_res = k99*std_ehat\n",
    "\n",
    "    # Plot original data and fitted model\n",
    "    plt.figure(figsize = (15,5))\n",
    "    plt.plot(date, y_obs, 'k+',  label = 'Observations')\n",
    "    plt.plot(date, yhat,  label = 'Fitted model')\n",
    "    plt.fill_between(date, (yhat - confidence_interval_y), \n",
    "                     (yhat + confidence_interval_y), facecolor='orange',\n",
    "                     alpha=0.4, label = '99% Confidence Region')\n",
    "    plt.plot(date, true_disp, label = 'True model')\n",
    "    plt.legend()\n",
    "    plt.ylabel(data_type + ' Displacement [mm]')\n",
    "    plt.xlabel('Time')\n",
    "    plt.title(data_type + ' Observations and Fitted Model')\n",
    "\n",
    "    return ehat\n",
    "\n",
    "# ------------------------------------------------------------- #\n",
    "\n",
    "# True model which was used to generate the data (Monte Carlo simulations)\n",
    "\n",
    "k_true = 0.1\n",
    "R_true = -25 \n",
    "a_true = 180\n",
    "d0_true = 10\n",
    "\n",
    "disp_gnss  = (d0_true + R_true*(1 - np.exp(-days_gnss/a_true)) \n",
    "              + k_true*GW_at_GNSS_times) \n",
    "\n",
    "# Residuals and plots for GNSS incl. confidence bounds\n",
    "yhat_gnss = A_gnss @ xhat_gnss\n",
    "ehat_gnss_1 = plot_residual(dates_gnss, y_gnss, yhat_gnss,\n",
    "                             'GNSS', A_gnss, \n",
    "                             Sigma_Xhat_gnss, Sigma_Y_gnss, disp_gnss)\n",
    "\n",
    "# ------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions on the test set\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(days_gnss, yhat_gnss,  label = 'BLUE model', color='black')\n",
    "plt.plot(days_gnss, disp_gnss, label='True model', color='orange')\n",
    "\n",
    "# Get model predictions for a dense linspace in x\n",
    "x_plot = np.linspace(np.min(X),np.max(X),1000).reshape(-1,1)\n",
    "y_plot = new_model_gnss.predict(input_scaler.transform(x_plot))\n",
    "plt.plot(x_plot,target_scaler.inverse_transform(y_plot.reshape(-1,1)),color='purple',linewidth=3,label='Network')\n",
    "\n",
    "plt.title('Obvserved vs Predicted Values')\n",
    "plt.ylabel('Displacement [mm]')\n",
    "plt.xlabel('Time [days]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Task 3.1 (continued): Model Comparison Questions</b>  \n",
    "\n",
    "Using the figure produced above, compare the differences (write down a few observations about the characteristics of each method. Which model is do you think is better?\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that this is a very weird comparison, because BLUE has more information than the neural network uses! Can you remember why, and also remove this information to make the comparison \"fair\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Task 3.2: </b>  \n",
    "\n",
    "Determine what information is used by BLUE, and not the neural network. Then remove this piece of information from the BLUE model and re-run the analysis to redo the comparison.\n",
    "\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#AABAB2; color: black; vertical-align: middle; padding:15px; margin: 10px; border-radius: 10px; width: 95%\">\n",
    "<p>\n",
    "<b>Task 3.3 (BONUS!): </b>  \n",
    "\n",
    "The neural network can include the groundwater data quite easily! You should be able to do this with the code above by making the array 2D and including an extra input (feature).\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of notebook.**\n",
    "<h2 style=\"height: 60px\">\n",
    "</h2>\n",
    "<h3 style=\"position: absolute; display: flex; flex-grow: 0; flex-shrink: 0; flex-direction: row-reverse; bottom: 60px; right: 50px; margin: 0; border: 0\">\n",
    "    <style>\n",
    "        .markdown {width:100%; position: relative}\n",
    "        article { position: relative }\n",
    "    </style>\n",
    "    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n",
    "      <img alt=\"Creative Commons License\" style=\"border-width:; width:88px; height:auto; padding-top:10px\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n",
    "    </a>\n",
    "    <a rel=\"TU Delft\" href=\"https://www.tudelft.nl/en/ceg\">\n",
    "      <img alt=\"TU Delft\" style=\"border-width:0; width:100px; height:auto; padding-bottom:0px\" src=\"https://gitlab.tudelft.nl/mude/public/-/raw/main/tu-logo/TU_P1_full-color.png\"/>\n",
    "    </a>\n",
    "    <a rel=\"MUDE\" href=\"http://mude.citg.tudelft.nl/\">\n",
    "      <img alt=\"MUDE\" style=\"border-width:0; width:100px; height:auto; padding-bottom:0px\" src=\"https://gitlab.tudelft.nl/mude/public/-/raw/main/mude-logo/MUDE_Logo-small.png\"/>\n",
    "    </a>\n",
    "    \n",
    "</h3>\n",
    "<span style=\"font-size: 75%\">\n",
    "&copy; Copyright 2023 <a rel=\"MUDE Team\" href=\"https://studiegids.tudelft.nl/a101_displayCourse.do?course_id=65595\">MUDE Teaching Team</a> TU Delft. This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
